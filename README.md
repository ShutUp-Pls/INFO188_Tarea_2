# Multiplicaci√≥n de Matrices: Comparaci√≥n CPU vs GPU

## Descripci√≥n

Este proyecto implementa y compara tres algoritmos de multiplicaci√≥n de matrices para matrices cuadradas (N√óN):

1. **CPU Multicore (OpenMP)**: Implementaci√≥n paralela en CPU utilizando OpenMP con paralelizaci√≥n del bucle externo mediante `#pragma omp parallel for`.

2. **GPU B√°sica (Global Memory)**: Implementaci√≥n en GPU CUDA utilizando memoria global. Cada thread calcula un elemento de la matriz resultado accediendo directamente a memoria global.

3. **GPU Shared Memory (Tiled)**: Implementaci√≥n optimizada en GPU utilizando memoria compartida y t√©cnica de tiling. Los datos se cargan en bloques (tiles) a memoria compartida para reducir accesos a memoria global y mejorar el rendimiento.

El objetivo es analizar el rendimiento de cada enfoque y calcular el speedup de las implementaciones GPU respecto a la versi√≥n CPU.

---

## Hardware Utilizado

- **CPU**: AMD Ryzen 5 3600 (6 n√∫cleos, 12 threads)
- **GPU**: NVIDIA GeForce RTX 2070 Super (Compute Capability 7.5)
- **Sistema Operativo**: Linux

---

## Estructura del Proyecto

```
.
‚îú‚îÄ‚îÄ matmul_compa.cu       # C√≥digo fuente CUDA/C++
‚îú‚îÄ‚îÄ Makefile              # Script de compilaci√≥n
‚îú‚îÄ‚îÄ benchmark_plot.py     # Script de automatizaci√≥n y visualizaci√≥n
‚îú‚îÄ‚îÄ README.md             # Este archivo
‚îú‚îÄ‚îÄ prog                  # Ejecutable (generado tras compilar)
‚îú‚îÄ‚îÄ grafico_tiempos_test1.png   # Gr√°fico de tiempos (generado por script)
‚îî‚îÄ‚îÄ grafico_speedup_test1.png   # Gr√°fico de speedup (generado por script)
```

---

## Instrucciones de Compilaci√≥n

### Requisitos Previos

- NVIDIA CUDA Toolkit (nvcc)
- Compilador compatible con OpenMP (gcc/g++)
- Python 3 con librer√≠as: numpy, matplotlib (para el script de benchmarking)

### Compilaci√≥n

Para compilar el programa, ejecuta:

```bash
make
```

Esto generar√° el ejecutable `prog` con las siguientes optimizaciones:

- `-O3`: Optimizaci√≥n m√°xima del compilador
- `-arch=sm_75`: Optimizaci√≥n espec√≠fica para arquitectura Turing (RTX 20xx)
- `-Xcompiler -fopenmp`: Soporte para OpenMP

### Limpiar archivos generados

```bash
make clean
```

### Recompilar desde cero

```bash
make rebuild
```

---

## Instrucciones de Ejecuci√≥n

### Ejecuci√≥n Manual

El programa acepta tres argumentos de l√≠nea de comandos:

```bash
./prog <n> <nt> <ALG>
```

**Par√°metros:**

- `n`: Tama√±o de la matriz (N√óN)
- `nt`: N√∫mero de threads CPU para OpenMP
- `ALG`: Algoritmo a ejecutar
  - `1` = CPU Multicore (OpenMP)
  - `2` = GPU B√°sica (Global Memory)
  - `3` = GPU Shared Memory (Tiled)

**Ejemplos:**

```bash
# Ejecutar versi√≥n CPU con matriz 1024√ó1024 usando 8 threads
./prog 1024 8 1

# Ejecutar versi√≥n GPU b√°sica con matriz 2048√ó2048
./prog 2048 8 2

# Ejecutar versi√≥n GPU optimizada con matriz 4096√ó4096
./prog 4096 8 3
```

### Ejecuci√≥n Automatizada (Benchmarking)

Para ejecutar todos los tests autom√°ticamente y generar los gr√°ficos comparativos:

```bash
python3 benchmark_plot.py
```

Este script:

1. Ejecuta el programa con diferentes tama√±os de matriz (512, 1024, 2048, 4096, 8192)
2. Prueba los 3 algoritmos para cada tama√±o
3. Captura y parsea los tiempos de ejecuci√≥n
4. Calcula los speedups de GPU respecto a CPU
5. Genera dos gr√°ficos:
   - `grafico_tiempos.png`: Comparaci√≥n de tiempos de ejecuci√≥n
   - `grafico_speedup.png`: Speedup de GPU sobre CPU
6. Muestra una tabla resumen con todos los resultados

**Nota**: Ajusta la variable `NUM_THREADS` en el script seg√∫n tu CPU.

---

## An√°lisis e Interpretaci√≥n

### Marco Te√≥rico

#### Versi√≥n CPU (OpenMP)

La implementaci√≥n CPU utiliza paralelizaci√≥n a nivel de threads mediante OpenMP. La directiva `#pragma omp parallel for` distribuye las iteraciones del bucle externo (filas de la matriz resultado) entre los threads disponibles. Aunque aprovecha m√∫ltiples n√∫cleos, est√° limitada por:

- Ancho de banda de memoria RAM
- N√∫mero de n√∫cleos f√≠sicos disponibles
- Overhead de sincronizaci√≥n entre threads

#### Versi√≥n GPU B√°sica (Global Memory)

Esta implementaci√≥n asigna un thread de GPU por cada elemento de la matriz resultado. Cada thread:

- Lee una fila completa de la matriz A
- Lee una columna completa de la matriz B
- Calcula el producto punto
- Escribe el resultado en memoria global

**Ventajas**: Paralelismo masivo (miles de threads simult√°neos)  
**Desventaja**: Alto tr√°fico de memoria global, que tiene alta latencia (400-800 ciclos de reloj)

#### Versi√≥n GPU Shared Memory (Tiled)

La optimizaci√≥n mediante tiling divide las matrices en bloques (tiles) y utiliza memoria compartida:

1. Cada bloque de threads carga un tile de A y B a memoria compartida
2. Sincroniza threads con `__syncthreads()`
3. Realiza productos parciales usando datos en memoria compartida (baja latencia: ~20 ciclos)
4. Repite para todos los tiles
5. Escribe resultado final a memoria global

**Ventajas**:

- Reducci√≥n dr√°stica de accesos a memoria global
- Reutilizaci√≥n de datos en memoria compartida (mucho m√°s r√°pida)
- Mejor uso de la jerarqu√≠a de memoria de la GPU

### Resultados Esperados

**Comportamiento de Tiempos**:

- La versi√≥n CPU deber√≠a mostrar crecimiento c√∫bico O(N¬≥) en el tiempo de ejecuci√≥n
- La versi√≥n GPU B√°sica deber√≠a ser significativamente m√°s r√°pida que CPU para matrices grandes
- La versi√≥n GPU Shared Memory deber√≠a superar a la GPU B√°sica, especialmente para matrices de gran tama√±o

**Speedup**:

- Se espera speedup creciente con el tama√±o de matriz (mejor amortizaci√≥n del overhead de transferencias)
- GPU Shared Memory deber√≠a alcanzar speedups de 10x-50x o m√°s respecto a CPU
- GPU B√°sica deber√≠a mostrar speedups de 5x-30x respecto a CPU
- La diferencia entre GPU B√°sica y Shared Memory se acent√∫a con matrices m√°s grandes

### Observaciones Reales

**Datos de Ejecuci√≥n Obtenidos**:

```
================================================================================
TABLA RESUMEN DE RESULTADOS
================================================================================
N        CPU (s)      GPU Global (s)   GPU Shared (s)   Speedup Global   Speedup Shared
--------------------------------------------------------------------------------
512      0.045938     0.000465         0.000306         98.79x           150.12x
1024     0.639745     0.003292         0.002065         194.33x          309.80x
2048     3.976043     0.028289         0.017205         140.55x          231.10x
4096     194.323232   0.199397         0.107544         974.55x          1806.92x
8192     N/A          1.436336         0.916615         N/A              N/A
================================================================================
```

**Tiempos de Ejecuci√≥n Medidos**:

- **N=512**: CPU 0.046s | GPU Global 0.0005s | GPU Shared 0.0003s
- **N=1024**: CPU 0.640s | GPU Global 0.003s | GPU Shared 0.002s
- **N=2048**: CPU 3.976s | GPU Global 0.028s | GPU Shared 0.017s
- **N=4096**: CPU 194.32s (~3.2 min) | GPU Global 0.199s | GPU Shared 0.108s
- **N=8192**: CPU Timeout (no complet√≥) | GPU Global 1.436s | GPU Shared 0.917s

**Speedups Observados**:

- **Speedup m√°ximo GPU B√°sica vs CPU**: 974.55x (en N=4096)
- **Speedup m√°ximo GPU Shared vs CPU**: 1806.92x (en N=4096)
- **Mejora de GPU Shared vs GPU B√°sica**: Consistentemente ~1.5x-2x m√°s r√°pida, alcanzando ~1.57x en N=8192

**An√°lisis Detallado de Resultados**:

#### 1. Colapso de Rendimiento en CPU para N=4096

El comportamiento m√°s notable ocurre en la transici√≥n de N=2048 a N=4096. Mientras que para N=2048 la CPU complet√≥ la tarea en aproximadamente 4 segundos, **para N=4096 el tiempo se dispar√≥ a ~194 segundos** (casi 50 veces m√°s tiempo).

Este salto masivo no es simplemente el crecimiento c√∫bico esperado O(N¬≥). La causa principal es **saturaci√≥n de la jerarqu√≠a de cach√©**:

- Para N=2048: Cada matriz ocupa 2048¬≤ √ó 4 bytes = ~16.8 MB. Con tres matrices, son ~50 MB totales.
- Para N=4096: Cada matriz ocupa 4096¬≤ √ó 4 bytes = ~67 MB. Con tres matrices, son ~201 MB totales.

El procesador AMD Ryzen 5 3600 tiene 32 MB de cach√© L3 compartida. **Para N=4096, las matrices exceden completamente la capacidad de la cach√© L3**, forzando accesos constantes a la RAM principal, que es 50-100 veces m√°s lenta que la cach√©. Esto provoca un cuello de botella severo en el ancho de banda de memoria, degradando dram√°ticamente el rendimiento.

#### 2. Speedup Extraordinario en N=4096

El caso de N=4096 demuestra el **speedup m√°s dram√°tico del benchmark: 1806.92x para GPU Shared Memory**. Esto significa que lo que la CPU tard√≥ casi 3 minutos y 15 segundos, la GPU lo resolvi√≥ en apenas **0.108 segundos (una d√©cima de segundo)**.

Este speedup extraordinario no solo se debe a la superioridad arquitect√≥nica de la GPU, sino a la **combinaci√≥n del colapso de la CPU por cache misses** y la **excelente gesti√≥n de memoria de la GPU**. La GPU, con su arquitectura dise√±ada para ancho de banda masivo (448 GB/s vs ~40 GB/s de RAM del sistema), no sufre la misma penalizaci√≥n al trabajar con datasets grandes.

#### 3. Superioridad Consistente de Shared Memory (Alg 3) sobre Global Memory (Alg 2)

En todos los tama√±os de matriz, la versi√≥n optimizada con memoria compartida supera a la versi√≥n b√°sica:

- **N=512**: 0.306 ms vs 0.465 ms (1.52x m√°s r√°pida)
- **N=1024**: 2.065 ms vs 3.292 ms (1.59x m√°s r√°pida)
- **N=2048**: 17.205 ms vs 28.289 ms (1.64x m√°s r√°pida)
- **N=4096**: 107.544 ms vs 199.397 ms (1.85x m√°s r√°pida)
- **N=8192**: 916.615 ms vs 1436.336 ms (1.57x m√°s r√°pida)

Esta ventaja se explica por la **t√©cnica de tiling y el uso de memoria compartida**:

1. **Reducci√≥n de accesos a memoria global**: En la versi√≥n b√°sica, cada elemento del resultado requiere N lecturas de memoria global (N accesos para A[fila] y N accesos para B[columna]). Con tiling, los datos se cargan una vez en tiles a memoria compartida y se reutizan m√∫ltiples veces.

2. **Mayor ancho de banda efectivo**: La memoria compartida tiene un ancho de banda ~10x superior a la memoria global (varios TB/s vs ~448 GB/s), con latencia 20x menor (~20 ciclos vs ~400-800 ciclos).

3. **Mejor coalescencia de accesos**: El patr√≥n de tiling permite accesos coalesced (contiguos) a memoria global, maximizando el throughput del bus de memoria.

La mejora se vuelve m√°s pronunciada en N=4096 (1.85x), donde el mayor volumen de datos amplifica el beneficio de reducir tr√°fico a memoria global.

#### 4. Timeout de CPU en N=8192: GPU como √önica Soluci√≥n Pr√°ctica

Para N=8192, la versi√≥n CPU **no complet√≥ la ejecuci√≥n en un tiempo razonable** (timeout), mientras que:

- GPU Global Memory: **1.436 segundos**
- GPU Shared Memory: **0.917 segundos**

Una matriz de 8192√ó8192 elementos float ocupa ~268 MB. Con tres matrices son ~804 MB, muy por encima de cualquier nivel de cach√© del CPU. El problema se vuelve completamente dominado por el ancho de banda de RAM, haciendo la ejecuci√≥n en CPU impr√°ctica.

En contraste, la GPU maneja este problema con facilidad gracias a:

- **Paralelismo masivo**: Miles de threads procesando simult√°neamente
- **Arquitectura optimizada para throughput**: Dise√±ada para mover grandes vol√∫menes de datos
- **VRAM de alta velocidad**: 448 GB/s en la RTX 2070 Super vs ~40 GB/s de RAM DDR4

Este resultado demuestra que **para problemas de √°lgebra lineal a gran escala, la GPU no es solo "m√°s r√°pida", sino frecuentemente la √∫nica opci√≥n viable** en t√©rminos pr√°cticos.

#### Conclusiones Clave

1. ‚úÖ **Los resultados confirman la teor√≠a**: Comportamiento c√∫bico, jerarqu√≠a de memoria cr√≠tica, y ventajas de tiling verificadas experimentalmente.

2. üìà **Mejor speedup en N=4096**: 1806x, donde coinciden la saturaci√≥n de cach√© CPU y la eficiencia √≥ptima de GPU.

3. üéØ **Factores limitantes**:

   - **CPU**: Ancho de banda de RAM y tama√±o de cach√© L3
   - **GPU B√°sica**: Latencia y ancho de banda de memoria global
   - **GPU Shared**: Overhead de sincronizaci√≥n y tama√±o de memoria compartida por bloque

4. üîß **Impacto del tile size (16√ó16)**: √ìptimo para este hardware (Compute Capability 7.5), balanceando ocupancia de memoria compartida (48 KB/SM) con reutilizaci√≥n de datos. Tiles m√°s grandes aumentar√≠an colisiones de banco de memoria compartida; tiles m√°s peque√±os reducir√≠an la reutilizaci√≥n.

---

## Visualizaci√≥n de Resultados

### Gr√°fico de Tiempos de Ejecuci√≥n

![Gr√°fico de Tiempos](grafico_tiempos_test1.png)

_Este gr√°fico muestra los tiempos de ejecuci√≥n (en segundos) de los tres algoritmos para diferentes tama√±os de matriz. La escala logar√≠tmica en el eje Y facilita la comparaci√≥n entre implementaciones con √≥rdenes de magnitud de diferencia._

### Gr√°fico de Speedup

![Gr√°fico de Speedup](grafico_speedup_test1.png)

_Este gr√°fico ilustra la aceleraci√≥n (speedup) de las versiones GPU respecto a la implementaci√≥n CPU OpenMP. Un speedup mayor indica mejor rendimiento relativo de la GPU._

---

## Detalles de Implementaci√≥n

### Kernel GPU B√°sico

```cuda
__global__ void kernel_matmul(int n, float *a, float *b, float *c) {
    int tx = blockIdx.x * blockDim.x + threadIdx.x;
    int ty = blockIdx.y * blockDim.y + threadIdx.y;
    float sum = 0.0f;

    if(tx < n && ty < n) {
        for(int k = 0; k < n; ++k) {
            sum += a[ty * n + k] * b[k * n + tx];
        }
        c[ty * n + tx] = sum;
    }
}
```

### Kernel GPU Shared Memory

- Utiliza tiles de 16√ó16 elementos
- Sincronizaci√≥n con `__syncthreads()` despu√©s de cada carga
- Memoria compartida: `__shared__ float tile_a[16][16]`

### Configuraci√≥n de Grid y Bloques

- Bloques de 16√ó16 threads (256 threads por bloque)
- Grid dimensionado din√°micamente seg√∫n N: `gridDim = (N/16, N/16)`

---

## Medici√≥n de Tiempos

- **CPU**: Utiliza `omp_get_wtime()` de OpenMP
- **GPU**: Utiliza eventos CUDA (`cudaEvent_t`) para medir √∫nicamente el tiempo de ejecuci√≥n del kernel, excluyendo transferencias de memoria

---

## Referencias

- NVIDIA CUDA Programming Guide
- OpenMP Specification
- Curso de Programaci√≥n Paralela - Universidad Austral de Chile

---

## Autor

Tarea Universitaria - Paradigmas de Programaci√≥n  
Universidad Austral de Chile  
Fecha: 19 de diciembre de 2025

---

## Licencia

Este c√≥digo es material acad√©mico desarrollado con fines educativos.
