### Miembros del equipo:
- Nicol√°s Sandoval Jerez
- Marco Delgado Salda√±a
- Sebastian Fuentes C.
- Mart√≠n Jaque

# Multiplicaci√≥n de Matrices: Comparaci√≥n CPU vs GPU

## Descripci√≥n

Este proyecto implementa y compara tres algoritmos de multiplicaci√≥n de matrices para matrices cuadradas (N√óN):

1. **CPU Multicore (OpenMP)**: Implementaci√≥n paralela en CPU utilizando OpenMP con paralelizaci√≥n del bucle externo mediante `#pragma omp parallel for`.

2. **GPU B√°sica (Global Memory)**: Implementaci√≥n en GPU CUDA utilizando memoria global. Cada thread calcula un elemento de la matriz resultado accediendo directamente a memoria global.

3. **GPU Shared Memory (Tiled)**: Implementaci√≥n optimizada en GPU utilizando memoria compartida y t√©cnica de tiling. Los datos se cargan en bloques (tiles) a memoria compartida para reducir accesos a memoria global y mejorar el rendimiento.

4. **[Extra] GPU Tensor Cores (WMMA)**: Implementaci√≥n de alto rendimiento que aprovecha las unidades de hardware especializadas (Tensor Cores) presentes en arquitecturas modernas (Volta+). Utiliza la API `nvcuda::wmma` para realizar operaciones matriciales a nivel de warp con precisi√≥n mixta (entradas en `half`, acumulaci√≥n en `float`), logrando una aceleraci√≥n aritm√©tica significativamente superior a los n√∫cleos CUDA est√°ndar.

El objetivo es analizar el rendimiento de cada enfoque y calcular el speedup de las implementaciones GPU respecto a la versi√≥n CPU.

---

## Hardware Utilizado

**Implementaci√≥n y conclusiones principales (Test sin Tensor Cores)**
- **CPU**: AMD Ryzen 5 3600 (6 n√∫cleos, 12 threads)
- **GPU**: NVIDIA GeForce RTX 2070 Super (Compute Capability 7.5)
- **Sistema Operativo**: Linux

**Implementaci√≥n y conclusiones extras (Test con Tensor Cores)**
- **CPU**: Intel i7-11800H (8 nucleos, 16 threads)
- **GPU**: NVIDIA GeForce RTX 3060 Laptop (Compute Capability 8.6)
- **Sistema Operativo**: Linux (Ubuntu 24.04.3 LTS)
---

## Estructura del Proyecto

```
‚îú‚îÄ‚îÄ plots                       # Carpeta con gr√°ficos generados
‚îÇ   ‚îú‚îÄ‚îÄ speedup_test_tensors.png
‚îÇ   ‚îú‚îÄ‚îÄ speedup_test_principal.png
‚îÇ   ‚îú‚îÄ‚îÄ tiempos_test_tensors.png
‚îÇ   ‚îî‚îÄ‚îÄ tiempos_test_principal.png
‚îú‚îÄ‚îÄ benchmark_plot.py           # Script de automatizaci√≥n y visualizaci√≥n
‚îú‚îÄ‚îÄ LICENSE                     # Archivo de licencia
‚îú‚îÄ‚îÄ Makefile                    # Script de compilaci√≥n
‚îú‚îÄ‚îÄ matmul_compa.cu             # C√≥digo fuente CUDA/C++
‚îú‚îÄ‚îÄ prog                        # Ejecutable (generado tras compilar)
‚îî‚îÄ‚îÄ README.md                   # Este archivo
```
---

## Instrucciones de Compilaci√≥n y Ejecuci√≥n

### 1. Verificaci√≥n de Entorno (Recomendado)

Antes de compilar, puedes verificar que tienes todas las herramientas y librer√≠as necesarias (compilador CUDA, drivers, Python, Numpy, Matplotlib) ejecutando:

```bash
make check
```

### 2. Compilaci√≥n

Para compilar el programa, ejecuta:

```bash
make
```

El script detectar√° autom√°ticamente tu GPU y generar√° el ejecutable `prog` con las siguientes banderas:

* `-O3`: Optimizaci√≥n m√°xima del compilador.
* `-Xcompiler -fopenmp`: Soporte para paralelismo en CPU (OpenMP).
* **Arquitectura Din√°mica (`-arch`):**
* **`sm_75`**: Si detecta una RTX 2070/Turing.
* **`sm_86`**: Si detecta una RTX 3060/Ampere.
* **`sm_60`**: Fallback por defecto (Pascal) para compatibilidad general.



> **Nota:** Si deseas ver qu√© arquitectura se detect√≥ sin compilar, usa `make info`.

### 3. Limpiar archivos

Para eliminar el ejecutable compilado:

```bash
make clean
```

### 4. Recompilar desde cero

Para limpiar todo y volver a compilar inmediatamente:

```bash
make rebuild
```
---

## Instrucciones de Ejecuci√≥n

### Ejecuci√≥n Manual

El programa acepta tres argumentos de l√≠nea de comandos:

```bash
./prog <n> <nt> <ALG>
```

**Par√°metros:**

- `n`: Tama√±o de la matriz (N√óN)
- `nt`: N√∫mero de threads CPU para OpenMP
- `ALG`: Algoritmo a ejecutar
  - `1` = CPU Multicore (OpenMP)
  - `2` = GPU B√°sica (Global Memory)
  - `3` = GPU Shared Memory (Tiled)
  - `4` = GPU Tensor Cores (WMMA)

**Ejemplos:**

```bash
# Ejecutar versi√≥n CPU con matriz 1024√ó1024 usando 8 threads
./prog 1024 8 1

# Ejecutar versi√≥n GPU b√°sica con matriz 2048√ó2048
./prog 2048 8 2

# Ejecutar versi√≥n GPU optimizada con matriz 4096√ó4096
./prog 4096 8 3

# Ejecutar versi√≥n GPU con tensor cores con matriz 8192x8192
./prog 8192 8 4
```

### Ejecuci√≥n Automatizada (Benchmarking)

Para ejecutar todos los tests autom√°ticamente y generar los gr√°ficos comparativos:

```bash
python3 benchmark_plot.py
```

Este script:

1. Ejecuta el programa con diferentes tama√±os de matriz (512, 1024, 2048, 4096, 8192)
2. Prueba los 4 algoritmos para cada tama√±o
3. Captura y parsea los tiempos de ejecuci√≥n
4. Calcula los speedups de GPU respecto a CPU
5. Genera dos gr√°ficos:
   - `grafico_tiempos.png`: Comparaci√≥n de tiempos de ejecuci√≥n
   - `grafico_speedup.png`: Speedup de GPU sobre CPU
6. Muestra una tabla resumen con todos los resultados

**Nota**: Ajusta la variable `NUM_THREADS` en el script seg√∫n tu CPU.

---

## An√°lisis e Interpretaci√≥n

### Marco Te√≥rico

#### Versi√≥n CPU (OpenMP)

La implementaci√≥n CPU utiliza paralelizaci√≥n a nivel de threads mediante OpenMP. La directiva `#pragma omp parallel for` distribuye las iteraciones del bucle externo (filas de la matriz resultado) entre los threads disponibles. Aunque aprovecha m√∫ltiples n√∫cleos, est√° limitada por:

- Ancho de banda de memoria RAM
- N√∫mero de n√∫cleos f√≠sicos disponibles
- Overhead de sincronizaci√≥n entre threads

#### Versi√≥n GPU B√°sica (Global Memory)

Esta implementaci√≥n asigna un thread de GPU por cada elemento de la matriz resultado. Cada thread:

- Lee una fila completa de la matriz A
- Lee una columna completa de la matriz B
- Calcula el producto punto
- Escribe el resultado en memoria global

**Ventajas**: Paralelismo masivo (miles de threads simult√°neos)  
**Desventaja**: Alto tr√°fico de memoria global, que tiene alta latencia (400-800 ciclos de reloj)

#### Versi√≥n GPU Shared Memory (Tiled)

La optimizaci√≥n mediante tiling divide las matrices en bloques (tiles) y utiliza memoria compartida:

1. Cada bloque de threads carga un tile de A y B a memoria compartida
2. Sincroniza threads con `__syncthreads()`
3. Realiza productos parciales usando datos en memoria compartida (baja latencia: ~20 ciclos)
4. Repite para todos los tiles
5. Escribe resultado final a memoria global

**Ventajas**:

- Reducci√≥n dr√°stica de accesos a memoria global
- Reutilizaci√≥n de datos en memoria compartida (mucho m√°s r√°pida)
- Mejor uso de la jerarqu√≠a de memoria de la GPU

#### [Extra] Versi√≥n GPU Tensor Cores (WMMA)

Esta implementaci√≥n utiliza las unidades de hardware especializadas (Tensor Cores) disponibles en arquitecturas Volta y superiores. A diferencia del modelo est√°ndar donde cada thread calcula un escalar, aqu√≠ se utiliza la API `nvcuda::wmma` para realizar operaciones cooperativas a nivel de **warp** (grupos de 32 threads).

* Los datos de entrada (matrices A y B) se convierten y cargan en precisi√≥n media (`half` / FP16), reduciendo la carga en memoria.
* Un warp completo colabora para cargar fragmentos y ejecutar una instrucci√≥n de hardware que calcula un tile de 16x16x16 en un solo paso.
* Aunque las entradas son FP16, la suma se acumula en precisi√≥n simple (`float` / FP32) para evitar desbordamientos num√©ricos.

**Ventajas**:

* Rendimiento aritm√©tico masivo.
* Reducci√≥n del ancho de banda necesario al leer datos de 16 bits.
* Ejecuci√≥n de multiplicaciones matriciales complejas en ciclos de reloj especializados.

**Desventaja**: Requiere conversi√≥n de tipos de datos y hardware espec√≠fico; la granularidad m√≠nima es fija (tiles de 16x16).

### Resultados Esperados

**Comportamiento de Tiempos**:

- La versi√≥n CPU deber√≠a mostrar crecimiento c√∫bico O(N¬≥) en el tiempo de ejecuci√≥n
- La versi√≥n GPU B√°sica deber√≠a ser significativamente m√°s r√°pida que CPU para matrices grandes
- La versi√≥n GPU Shared Memory deber√≠a superar a la GPU B√°sica, especialmente para matrices de gran tama√±o
- *[Extra]* La versi√≥n GPU Tensor Cores deber√≠a mostrar los tiempos m√°s bajos absolutos, manteniendose constantemente por debajo de las implementaciones anteriores.

**Speedup**:

- Se espera speedup creciente con el tama√±o de matriz (mejor amortizaci√≥n del overhead de transferencias)
- GPU Shared Memory deber√≠a alcanzar speedups de 10x-50x o m√°s respecto a CPU
- GPU B√°sica deber√≠a mostrar speedups de 5x-30x respecto a CPU
- La diferencia entre GPU B√°sica y Shared Memory se acent√∫a con matrices m√°s grandes
- *[Extra]* GPU Tensor Cores deber√≠a dominar la comparativa, superando incluso a la versi√≥n Shared Memory, gracias a la aceleraci√≥n de hardware dedicada.

### Observaciones Reales

**Datos de Ejecuci√≥n Obtenidos (Implementaci√≥n principal [Sin Tensor Cores])**:

```
================================================================================
TABLA RESUMEN DE RESULTADOS
================================================================================
N        CPU (s)      GPU Global (s)   GPU Shared (s)   Speedup Global   Speedup Shared
--------------------------------------------------------------------------------
512      0.045938     0.000465         0.000306         98.79x           150.12x
1024     0.639745     0.003292         0.002065         194.33x          309.80x
2048     3.976043     0.028289         0.017205         140.55x          231.10x
4096     194.323232   0.199397         0.107544         974.55x          1806.92x
8192     N/A          1.436336         0.916615         N/A              N/A
================================================================================
```

**An√°lisis Detallado de Resultados**:

#### 1. Colapso de Rendimiento en CPU para N=4096

El comportamiento m√°s notable ocurre en la transici√≥n de N=2048 a N=4096. Mientras que para N=2048 la CPU complet√≥ la tarea en aproximadamente 4 segundos, **para N=4096 el tiempo se dispar√≥ a ~194 segundos** (casi 50 veces m√°s tiempo).

Este salto masivo no es simplemente el crecimiento c√∫bico esperado O(N¬≥). La causa principal es **saturaci√≥n de la jerarqu√≠a de cach√©**:

- Para N=2048: Cada matriz ocupa 2048¬≤ √ó 4 bytes = ~16.8 MB. Con tres matrices, son ~50 MB totales.
- Para N=4096: Cada matriz ocupa 4096¬≤ √ó 4 bytes = ~67 MB. Con tres matrices, son ~201 MB totales.

El procesador AMD Ryzen 5 3600 tiene 32 MB de cach√© L3 compartida. **Para N=4096, las matrices exceden completamente la capacidad de la cach√© L3**, forzando accesos constantes a la RAM principal, que es 50-100 veces m√°s lenta que la cach√©. Esto provoca un cuello de botella severo en el ancho de banda de memoria, degradando dram√°ticamente el rendimiento.

#### 2. Speedup Extraordinario en N=4096

El caso de N=4096 demuestra el **speedup m√°s dram√°tico del benchmark: 1806.92x para GPU Shared Memory**. Esto significa que lo que la CPU tard√≥ casi 3 minutos y 15 segundos, la GPU lo resolvi√≥ en apenas **0.108 segundos (una d√©cima de segundo)**.

Este speedup extraordinario no solo se debe a la superioridad arquitect√≥nica de la GPU, sino a la **combinaci√≥n del colapso de la CPU por cache misses** y la **excelente gesti√≥n de memoria de la GPU**. La GPU, con su arquitectura dise√±ada para ancho de banda masivo (448 GB/s vs ~40 GB/s de RAM del sistema), no sufre la misma penalizaci√≥n al trabajar con datasets grandes.

#### 3. Superioridad Consistente de Shared Memory (Alg 3) sobre Global Memory (Alg 2)

En todos los tama√±os de matriz, la versi√≥n optimizada con memoria compartida supera a la versi√≥n b√°sica:

- **N=512**: 0.306 ms vs 0.465 ms (1.52x m√°s r√°pida)
- **N=1024**: 2.065 ms vs 3.292 ms (1.59x m√°s r√°pida)
- **N=2048**: 17.205 ms vs 28.289 ms (1.64x m√°s r√°pida)
- **N=4096**: 107.544 ms vs 199.397 ms (1.85x m√°s r√°pida)
- **N=8192**: 916.615 ms vs 1436.336 ms (1.57x m√°s r√°pida)

Esta ventaja se explica por la **t√©cnica de tiling y el uso de memoria compartida**:

1. **Reducci√≥n de accesos a memoria global**: En la versi√≥n b√°sica, cada elemento del resultado requiere N lecturas de memoria global (N accesos para A[fila] y N accesos para B[columna]). Con tiling, los datos se cargan una vez en tiles a memoria compartida y se reutizan m√∫ltiples veces.

2. **Mayor ancho de banda efectivo**: La memoria compartida tiene un ancho de banda ~10x superior a la memoria global (varios TB/s vs ~448 GB/s), con latencia 20x menor (~20 ciclos vs ~400-800 ciclos).

3. **Mejor coalescencia de accesos**: El patr√≥n de tiling permite accesos coalesced (contiguos) a memoria global, maximizando el throughput del bus de memoria.

La mejora se vuelve m√°s pronunciada en N=4096 (1.85x), donde el mayor volumen de datos amplifica el beneficio de reducir tr√°fico a memoria global.

#### 4. Timeout de CPU en N=8192: GPU como √önica Soluci√≥n Pr√°ctica

Para N=8192, la versi√≥n CPU **no complet√≥ la ejecuci√≥n en un tiempo razonable** (timeout), mientras que:

- GPU Global Memory: **1.436 segundos**
- GPU Shared Memory: **0.917 segundos**

Una matriz de 8192√ó8192 elementos float ocupa ~268 MB. Con tres matrices son ~804 MB, muy por encima de cualquier nivel de cach√© del CPU. El problema se vuelve completamente dominado por el ancho de banda de RAM, haciendo la ejecuci√≥n en CPU impr√°ctica.

En contraste, la GPU maneja este problema con facilidad gracias a:

- **Paralelismo masivo**: Miles de threads procesando simult√°neamente
- **Arquitectura optimizada para throughput**: Dise√±ada para mover grandes vol√∫menes de datos
- **VRAM de alta velocidad**: 448 GB/s en la RTX 2070 Super vs ~40 GB/s de RAM DDR4

Este resultado demuestra que **para problemas de √°lgebra lineal a gran escala, la GPU no es solo "m√°s r√°pida", sino frecuentemente la √∫nica opci√≥n viable** en t√©rminos pr√°cticos.

#### Conclusiones Clave

1. ‚úÖ **Los resultados confirman la teor√≠a**: Comportamiento c√∫bico, jerarqu√≠a de memoria cr√≠tica, y ventajas de tiling verificadas experimentalmente.

2. üìà **Mejor speedup en N=4096**: 1806x, donde coinciden la saturaci√≥n de cach√© CPU y la eficiencia √≥ptima de GPU.

3. üéØ **Factores limitantes**:

   - **CPU**: Ancho de banda de RAM y tama√±o de cach√© L3
   - **GPU B√°sica**: Latencia y ancho de banda de memoria global
   - **GPU Shared**: Overhead de sincronizaci√≥n y tama√±o de memoria compartida por bloque

4. üîß **Impacto del tile size (16√ó16)**: √ìptimo para este hardware (Compute Capability 7.5), balanceando ocupancia de memoria compartida (48 KB/SM) con reutilizaci√≥n de datos. Tiles m√°s grandes aumentar√≠an colisiones de banco de memoria compartida; tiles m√°s peque√±os reducir√≠an la reutilizaci√≥n.

**Datos de Ejecuci√≥n Obtenidos (Implementaci√≥n extra [Con Tensor Cores])**:

```
================================================================================================================
TABLA RESUMEN DE RESULTADOS
================================================================================================================
N        CPU (s)     GPU Global (s)  GPU Shared (s)  GPU Tensor (s)  Sp Global   Sp Shared   Sp Tensor
----------------------------------------------------------------------------------------------------------------
512      0.031475    0.000448        0.000323        0.000061        70.26x      97.45x      515.98x
1024     0.224134    0.002630        0.002074        0.000367        85.22x      108.07x     610.72x
2048     5.741328    0.027602        0.021755        0.003891        208.00x     263.91x     1475.54x
4096     58.091792   0.249227        0.179522        0.024757        233.09x     323.59x     2346.48x
8192     N/A         1.795710        1.305296        0.237692        N/A         N/A         N/A
================================================================================================================

```

**An√°lisis Detallado de Resultados**:

*Sumado al an√°lisis anterior sobre el colapso de la CPU por saturaci√≥n de cach√© y las mejoras de Shared Memory (el cual se mantiene v√°lido), la incorporaci√≥n de los Tensor Cores introduce un nuevo paradigma de rendimiento:*

#### 1. Salto Generacional de Rendimiento (Tensor Cores)

La diferencia entre el algoritmo de Shared Memory (Alg 3) y Tensor Cores (Alg 4) es abismal. Mientras que pasar de Global a Shared Memory ofrec√≠a una mejora de ~1.5x, **pasar de Shared Memory a Tensor Cores ofrece una mejora de entre 5x y 7.2x**.

* En **N=4096**, la GPU Shared tard√≥ 179ms, mientras que los Tensor Cores pulverizaron la tarea en **24ms**.
* En **N=8192**, donde incluso la memoria compartida requiri√≥ 1.3 segundos, los Tensor Cores completaron la operaci√≥n en **0.23 segundos**.

Esto demuestra que el hardware especializado (instrucciones WMMA) rompe la linealidad de mejora que se obtiene solo optimizando software sobre n√∫cleos CUDA est√°ndar.

#### 2. Eficiencia de Ancho de Banda y Precisi√≥n Mixta

El rendimiento extremo de los Tensor Cores (Alg 4) se explica no solo por la capacidad de c√≥mputo, sino por la reducci√≥n de la presi√≥n en la memoria:

1. **Entrada FP16 (Half Precision)**: Al convertir las matrices A y B a `half` antes del c√≥mputo, se reduce a la mitad el ancho de banda necesario para leer los datos desde la memoria global y la cach√© L2, comparado con los `float` (FP32) usados en los algoritmos 2 y 3.
2. **Aritm√©tica Matricial Densa**: A diferencia de los n√∫cleos CUDA que realizan operaciones escalar-vector (FMA), los Tensor Cores ejecutan una multiplicaci√≥n de matrices 4x4, 8x8 o 16x16 en un solo ciclo de reloj especializado por warp.

#### 3. Dominio Absoluto del Speedup (2346x)

El speedup de **2346.48x** observado en N=4096 es un hito. Significa que la tarea que a la CPU le tom√≥ casi un minuto, la GPU usando Tensor Cores la realiz√≥ en el tiempo que dura un parpadeo (~24ms). Esto confirma que para cargas de trabajo de aprendizaje profundo o simulaciones cient√≠ficas densas, el uso de Tensor Cores no es opcional, sino obligatorio para obtener rendimiento en tiempo real.

#### Conclusiones Clave (Considerando Tensor Cores)

1. ‚úÖ **An√°lisis previo ratificado**: Se mantienen las conclusiones sobre las limitaciones de CPU y la eficacia del Tiling.
2. üöÄ **Supremac√≠a de Hardware**: La implementaci√≥n de **Tensor Cores (Alg 4) es la clara vencedora**, superando a la versi√≥n Shared Memory por un factor de **~7x** y a la CPU por m√°s de **2300x**.
3. üìâ **Escalabilidad Extrema**: En N=8192, Tensor Cores es la √∫nica implementaci√≥n que se mantiene en la escala de "sub-segundo" (0.23s), haciendo viable el procesamiento de matrices masivas que ser√≠an inmanejables por m√©todos tradicionales.
4. üíé **Valor de la Especializaci√≥n**: Los resultados validan el uso de precisi√≥n mixta (`half` input / `float` accumulate) como la estrategia √≥ptima para maximizar el throughput aritm√©tico en GPUs modernas (Volta/Turing+).

---

## Visualizaci√≥n de Resultados

### Gr√°fico de Tiempos de Ejecuci√≥n

![Gr√°fico de Tiempos Sin Tensor](plots/tiempos_test_principal.png)

_Este gr√°fico muestra los tiempos de ejecuci√≥n (en segundos) de los tres algoritmos para diferentes tama√±os de matriz. La escala logar√≠tmica en el eje Y facilita la comparaci√≥n entre implementaciones con √≥rdenes de magnitud de diferencia._

![Gr√°fico de Tiempos Con Tensor](plots/tiempos_test_tensors.png)

*Este gr√°fico muestra los tiempos de ejecuci√≥n (en escala logar√≠tmica) de los cuatro algoritmos. Se observa c√≥mo la implementaci√≥n **CPU** crece exponencialmente hasta el timeout. Las implementaciones **GPU Global** y **GPU Shared** mantienen un rendimiento s√≥lido, pero la **GPU Tensor Cores** se separa visualmente del grupo, manteni√©ndose en el orden de los milisegundos incluso cuando los otros m√©todos entran en la escala de segundos.*

### Gr√°fico de Speedup

![Gr√°fico de Speedup](plots/speedup_test_principal.png)

_Este gr√°fico ilustra la aceleraci√≥n (speedup) de las versiones GPU respecto a la implementaci√≥n CPU OpenMP. Un speedup mayor indica mejor rendimiento relativo de la GPU._

![Gr√°fico de Speedup](plots/speedup_test_tensors.png)

*Este gr√°fico ilustra la aceleraci√≥n masiva obtenida. Mientras que las optimizaciones tradicionales de GPU (Global/Shared) logran speedups impresionantes de 200x-300x, la inclusi√≥n de **Tensor Cores** rompe la escala visual, alcanzando una aceleraci√≥n superior a **2300x**. Esto demuestra visualmente el cambio de paradigma que ofrece el hardware especializado frente a la computaci√≥n de prop√≥sito general.*

---

## Detalles de Implementaci√≥n

### 1. CPU Multicore (OpenMP)

* **Estrategia**: Paralelizaci√≥n del bucle exterior (filas de la matriz ) mediante `#pragma omp parallel for`.
* **Recursos**: Asignaci√≥n din√°mica de threads basada en los n√∫cleos l√≥gicos disponibles del procesador (Ryzen 5 3600).

### 2. GPU Global Memory (B√°sica)

* **Granularidad**: Mapeo 1:1 donde cada thread calcula un √∫nico elemento de la matriz resultado .
* **Memoria**: Lectura directa de matrices  y  desde memoria global sin cach√© programable intermedia.
* **Acceso**: Patr√≥n de acceso a memoria no optimizado para  (lectura por filas) y coalescente para  (lectura por columnas).

### 3. GPU Shared Memory (Tiled)

* **Estrategia**: Multiplicaci√≥n matricial por bloques (Tiling) para maximizar la localidad espacial de datos.
* **Memoria**: Uso de buffers en memoria compartida (`__shared__ float tile[16][16]`) para reducir el tr√°fico a VRAM.
* **Sincronizaci√≥n**: Barreras `__syncthreads()` para garantizar la carga completa de datos antes del c√≥mputo.

### 4. GPU Tensor Cores (WMMA)

* **API**: Implementaci√≥n mediante intr√≠nsecos `nvcuda::wmma` (Warp Matrix Multiply Accumulate).
* **Precisi√≥n Mixta**: Carga de datos en `half` precision (FP16) y acumulaci√≥n en `float` (FP32) para preservar rango din√°mico.
* **Colaboraci√≥n**: Operaci√≥n cooperativa a nivel de Warp (32 threads) procesando tiles de 16x16x16 en un solo paso de hardware.

### Configuraci√≥n de Ejecuci√≥n (Grid & Block)

| Algoritmo | Dimensi√≥n de Bloque | Dimensi√≥n de Grid | Observaci√≥n |
| --- | --- | --- | --- |
| **GPU Global** | 16  16 (256 threads) |  | Configuraci√≥n est√°ndar bidimensional. |
| **GPU Shared** | 16  16 (256 threads) |  | Coincide con el tama√±o del tile de memoria compartida. |
| **Tensor Cores** | 32  4 (128 threads/4 warps) | Variable seg√∫n  | Cada Warp procesa un tile independiente de 16x16. |

---

## Metodolog√≠a de Medici√≥n

Para garantizar la consistencia y justicia de las pruebas (fair comparison), se aislaron los tiempos de c√≥mputo puro:

* **CPU**: Medici√≥n mediante `omp_get_wtime()` (Wall-clock time de alta resoluci√≥n).
* **GPU**: Uso de eventos de hardware `cudaEvent_t`.
* Se excluyen expl√≠citamente los tiempos de transferencia de memoria Host  Device (`cudaMemcpy`).
* Se excluyen los tiempos de conversi√≥n de datos (`float`  `half`) en el caso de Tensor Cores.
* Sincronizaci√≥n expl√≠cita mediante `cudaEventSynchronize()` antes de detener el cron√≥metro.



---

## Referencias Bibliogr√°ficas

1. **NVIDIA Corporation**. (2024). *CUDA C++ Programming Guide v12.4*. Design Guide. Santa Clara, CA.
2. **NVIDIA Corporation**. (2018). *Programming Tensor Cores in CUDA 9*. NVIDIA Developer Blog.
3. **OpenMP Architecture Review Board**. (2018). *OpenMP Application Program Interface Version 5.0*.
4. **Sanders, J., & Kandrot, E.** (2010). *CUDA by Example: An Introduction to General-Purpose GPU Programming*. Addison-Wesley Professional.
5. **Universidad Austral de Chile**. (2025). *Material del Curso INFO188: Programaci√≥n en Paradigmas Funcional y Paralelo*. Facultad de Ciencias de la Ingenier√≠a.
---

## Autor

Tarea Universitaria - Paradigmas de Programaci√≥n  
Universidad Austral de Chile  
Fecha: 19 de diciembre de 2025